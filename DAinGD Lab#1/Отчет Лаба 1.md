# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #1 выполнил(а):
- Гаев А. Д. студент АТ-04, НМТ-213511.
- НМТ-213511.
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | # | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы
Ознакомиться с основными операторами зыка Python на примере реализации линейной регрессии.

## Задание 1
### Написать программы Hello World на Python и Unity.
Описание:
 - Для Python в отчете привести скриншоты с демонстрацией сохранения
документа google.colab на свой диск с запуском программы, выводящей
сообщение Hello World.
- Для Unity в отчете привести скришноты вывода сообщения Hello
World в консоль.

```py

print("Hello World!")

```
 - Получаем ![image](https://user-images.githubusercontent.com/90757310/195178704-62d1b853-5dae-4e1b-ada6-1b62ed7aa37d.png)


```C#

print("Hello World!")

```

- Получаем ![image](https://user-images.githubusercontent.com/90757310/195178793-3191674a-2109-4ff2-933c-fa087758fe89.png)



## Задание 2
### В разделе «ход работы» пошагово выполнить каждый пункт с описанием и примером реализации задачи по теме лабораторной работы.

Ход работы.

- 1 Произвести подготовку данных для работы с алгоритмом линейной
регрессии. 10 видов данных были установлены случайным образом, и
данные находились в линейной зависимости. Данные преобразуются в
формат массива, чтобы их можно было вычислить напрямую при
использовании умножения и сложения.

 
```py

import numpy as np
import matplotlib.pyplot as plt

%matplotlib inline

x = [3,21,22,34,54,34,55,67,89,99]
x = np.array(x)
y = [2,22,24,65,79,82,55,130,150,199]
y = np.array(y)

plt.scatter(x,y)
```
![image](https://user-images.githubusercontent.com/90757310/195178911-ff469c5a-d97c-4123-97cc-9ed3b01c8a26.png)

- Соответственно, в реальных задачах сбор данных будет происходить посредством встренных в саму игру плагинов. Они заполняют БД, которая уже в готовом виде предрасположена к выгрузке во внешние инструменты. Например Num'Py. В последней строке мы вывели визуализацию элементов массива. В документации к Num'Py выделена значимая его черта, которой мы не воспользовались, а именно - многоосевые массивы. т.е. нам не обязательно разбивать БД на множество последовательностей и обрабатывать их for-loops методом. У нас есть один n-мерный массив(n-осевой), над которым и будет идти работа.

```py
def model(a, b, x):
    return a*x + b

def loss_function(a, b, x, y):
    num = len(x)
    prediction=model(a,b,x)
    return (0.5/num) * (np.square(prediction-y)).sum()

def optimize(a,b,x,y):
    num = len(x)
    prediction = model(a,b,x)
    da = (1.0/num) * ((prediction -y)*x).sum()
    db = (1.0/num) * ((prediction -y).sum())
    a = a - Lr*da
    b = b - Lr*db
    return a, b

def iterate(a,b,x,y,times):
    for i in range(times):
        a,b = optimize(a,b,x,y)
    return a,b
```
![image](https://user-images.githubusercontent.com/90757310/195178980-3bce4d38-7fa3-4421-ae19-939acd81c27c.png)

- Модель линейной регрессии характеризует зависимость отдного параметра от другого. В тривиальном понимании - это график столбика А от сторки 1 в Google Sheets. Поскольку возможность выгрузки данных в таблицы более приспособлена под ручной труд, ей мы воспользоваться не можем в силу объемности наших исходных данных. Поэтому нам легче обучить нейросеть обработке наших данных путем скармлевания всевозможных референсов. А линейная регрессия - есть функция зависимости. 
- Функция потерь характеризует способность нейросети к прогнозированию модели в соотвествии с реальными полученными значениями. Т.е. определяет оторванность модели от дейсвтительности, или считает разность между прогнозируемым значением и полученным в действительности. Мы используем метот среднеквадратической ошибки (MSE) - Сумма всех квадратных разностей значений. 
MSE = 1/lenght E (y-i)^2
Визуально мы будем получать параболы в местах, где нейросеть дает неверный прогноз. Является инструментом отладки сети, т.е. можно посмотреть насколько эффективно сеть была всоосоздана и обучена, что важно, если мы хотим строить действительные модели на основании которых можно делать хоть какие-либо полезные выводы. И это касается всего.
- Функция оптимизации: метод градиентного спуска для нахождения частных производных w и b. Сам градиент - это вектор, так как наш массив двумерный, то он - поверхность. В этой поверхности есть локальный минимум и максимум. Тогда грандиент - это вектор, который указывает из точки локального минимума в точку локального максимума. Он показывает направление скорейшего возрастнания функции. метод градиентного спуска увеличивает коэфицент обучаемости модели, который является существенным параметром ее эффективности. 
```py
a = np.random.rand(1)
print(a)
b = np.random.rand(1)
print(b)
Lr = 0.000001

a,b = iterate(a,b,x,y,1)
prediction=model(a,b,x)
loss = loss_function(a, b, x, y)
print(a,b,loss)
plt.scatter(x,y)
plt.plot(x,prediction)
```
![image](https://user-images.githubusercontent.com/90757310/195179115-67cdbf92-f56d-410c-9da0-1c92f9c04556.png)

- Пошли интерации обучения. Ну тут и сказать то нечего. Выводятся два числа, первое из которых - коэфицент функции(характеризует тангенс угла наклона функции), второе - свободный член. При каждой итерации значения меняются в сторону более правдоподобных результатов, а значит чем больше интераций мы проведем, тем точнее выходные даные. Я умолчал о третьем числе - потерях. Та самая MSE о которой я рассказывал немногим ранее. Чем ближе она к 0, тем точнее предсказание модели. 
![image](https://user-images.githubusercontent.com/90757310/195179182-f2217fa5-b8a9-47d2-ad37-ddd585cbee41.png)
```py
a = np.random.rand(1)
print(a)
b = np.random.rand(1)
print(b)
Lr = 0.000001
## Пропустил визуальные итерации, они будут в скриншотах.(наверное)
a,b = iterate(a,b,x,y,10000)
prediction=model(a,b,x)
loss = loss_function(a, b, x, y)
print(a,b,loss)
plt.scatter(x,y)
plt.plot(x,prediction)
```
![image](https://user-images.githubusercontent.com/90757310/195179247-e19501e9-aae3-4f6d-acb9-5ea3bd6a3b18.png)


## Выводы

Вывели привет мир. Обучили модель на примере линейной регрессии, ознакомились в процессе со статьей из Habr "Математика для искусственных нейронных сетей для новичков" для понимания бытия MSE и посетили псайт Дмитрия Макарова "Машинное обучение", где наглядно(на примере двумерного массива(поверхности)) объяснялся алгоритм градиентого спуска.

| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
